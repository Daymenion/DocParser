version: '3.8'

services:
  doc-parser-vllm:
    image: doc-parser:vllm
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: doc-parser-vllm
    ports:
      - "9998:8000"  # expose vLLM OpenAI server
    environment:
      - PYTHONPATH=/workspace/weights:$PYTHONPATH
    volumes:
      - ../weights/DotsOCR:/workspace/weights/DotsOCR
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: /bin/bash
    command:
      - -c
      - |
        set -ex;
        echo '--- Starting vLLM server (doc-parser) ---';
        sed -i '/^from vllm\.entrypoints\.cli\.main import main/a from DotsOCR import modeling_dots_ocr_vllm' $(which vllm) || true;
        exec vllm serve /workspace/weights/DotsOCR \
          --tensor-parallel-size 1 \
          --gpu-memory-utilization 0.55 \
          --chat-template-content-format string \
          --served-model-name model \
          --trust-remote-code

  doc-parser-app:
    image: doc-parser:app
    build:
      context: ..
      dockerfile: docker/Dockerfile.app
    container_name: doc-parser-app
    depends_on:
      - doc-parser-vllm
    environment:
      - APP_PORT=7860
      - VLLM_HOST=doc-parser-vllm
      - VLLM_PORT=8000
    ports:
      - "7860:7860"  # FastAPI + Gradio
    volumes: []




